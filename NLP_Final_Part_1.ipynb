{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xe7VpNdqvpuk",
        "outputId": "aad2dbbf-1c39-462f-a3d5-d2393bc85543"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.11.12)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets transformers matplotlib\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yshaYcUMwBSt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb0b2bef-11f4-4c6c-8050-966f14616c15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Load WikiText-2\n",
        "ds = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
        "\n",
        "train_text = \"\\n\\n\".join(ds[\"train\"][\"text\"])\n",
        "val_text   = \"\\n\\n\".join(ds[\"validation\"][\"text\"])\n",
        "\n",
        "# Use GPT-2 tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "CJzCfR3lwKds",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2bffc92-9f8e-4ef1-a7d5-ed82170a0446"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2428601 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        }
      ],
      "source": [
        "def make_blocks(text, tokenizer, block_size=256):\n",
        "    ids = tokenizer(text)[\"input_ids\"]\n",
        "    n = len(ids)\n",
        "    n = (n // block_size) * block_size  # trim\n",
        "    ids = ids[:n]\n",
        "    data = torch.tensor(ids, dtype=torch.long)\n",
        "    return data.view(-1, block_size)\n",
        "\n",
        "block_size = 128  # fast for Colab\n",
        "\n",
        "train_blocks = make_blocks(train_text, tokenizer, block_size)\n",
        "val_blocks   = make_blocks(val_text, tokenizer, block_size)\n",
        "\n",
        "train_ds = TensorDataset(train_blocks[:, :-1], train_blocks[:, 1:])\n",
        "val_ds   = TensorDataset(val_blocks[:, :-1],   val_blocks[:, 1:])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "CBmb4terwNC2"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "val_loader   = DataLoader(val_ds, batch_size=batch_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "GDJYXRz-wRHP"
      },
      "outputs": [],
      "source": [
        "class LearnedPositionalEncoding(nn.Module):\n",
        "    def __init__(self, max_len, d_model):\n",
        "        super().__init__()\n",
        "        self.pe = nn.Embedding(max_len, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()\n",
        "        pos = torch.arange(T, device=x.device).unsqueeze(0)\n",
        "        return x + self.pe(pos)\n",
        "\n",
        "\n",
        "class SinusoidalPositionalEncoding(nn.Module):\n",
        "    def __init__(self, max_len, d_model):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:x.size(1)].unsqueeze(0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Wu4ATaTpwT5J"
      },
      "outputs": [],
      "source": [
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, lora_r=0):\n",
        "        super().__init__()\n",
        "        assert d_model % n_heads == 0\n",
        "\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = d_model // n_heads\n",
        "        self.attention_weights = None\n",
        "\n",
        "        # base linear layers\n",
        "        self.qkv = nn.Linear(d_model, 3 * d_model)\n",
        "        self.out_proj = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # wrap with LoRA if requested\n",
        "        if lora_r > 0:\n",
        "            self.qkv = LoRALinear(self.qkv, r=lora_r)\n",
        "            self.out_proj = LoRALinear(self.out_proj, r=lora_r)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()\n",
        "\n",
        "        qkv = self.qkv(x)\n",
        "        q, k, v = qkv.chunk(3, dim=-1)\n",
        "\n",
        "        # split into heads\n",
        "        def split(t):\n",
        "            return t.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        q, k, v = split(q), split(k), split(v)\n",
        "\n",
        "        # causal attention\n",
        "        att = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
        "        mask = torch.triu(torch.ones(T, T, device=x.device), diagonal=1)\n",
        "        att = att.masked_fill(mask == 1, float('-inf')).softmax(dim=-1)\n",
        "\n",
        "        self.attention_weights = att[0, 0].cpu().detach()\n",
        "\n",
        "        y = att @ v\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "\n",
        "        return self.out_proj(y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "qpyAgLRRxKHx"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, d_ff, dropout, lora_r=0):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(d_model)\n",
        "        self.attn = CausalSelfAttention(d_model, n_heads, lora_r=lora_r)\n",
        "        self.ln2 = nn.LayerNorm(d_model)\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(d_ff, d_model),\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.dropout(self.attn(self.ln1(x)))\n",
        "        x = x + self.dropout(self.ff(self.ln2(x)))\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "DFGdc-w5xPDF"
      },
      "outputs": [],
      "source": [
        "class TinyGPT(nn.Module):\n",
        "    def __init__(\n",
        "        self, vocab_size, d_model=256, n_layers=4,\n",
        "        n_heads=4, d_ff=1024, max_len=128,\n",
        "        dropout=0.1, pos_type=\"learned\",\n",
        "        lora_r=0\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.tok_emb = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_emb = LearnedPositionalEncoding(max_len, d_model) \\\n",
        "                       if pos_type==\"learned\" else \\\n",
        "                       SinusoidalPositionalEncoding(max_len, d_model)\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            Block(d_model, n_heads, d_ff, dropout, lora_r=lora_r)\n",
        "            for _ in range(n_layers)\n",
        "        ])\n",
        "\n",
        "        self.ln_f = nn.LayerNorm(d_model)\n",
        "        self.head = nn.Linear(d_model, vocab_size, bias=False)\n",
        "\n",
        "\n",
        "    def forward(self, x, targets=None):\n",
        "        h = self.tok_emb(x)\n",
        "        h = self.pos_emb(h)\n",
        "        for blk in self.blocks:\n",
        "            h = blk(h)\n",
        "        h = self.ln_f(h)\n",
        "        logits = self.head(h)\n",
        "\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = nn.functional.cross_entropy(\n",
        "                logits.view(-1, logits.size(-1)),\n",
        "                targets.view(-1),\n",
        "                ignore_index=-100\n",
        "            )\n",
        "        return logits, loss\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LoRALinear(nn.Module):\n",
        "    def __init__(self, linear: nn.Linear, r=8, alpha=16):\n",
        "        super().__init__()\n",
        "        self.linear = linear\n",
        "        self.r = r\n",
        "        self.alpha = alpha\n",
        "        self.scaling = alpha / r\n",
        "\n",
        "        # LoRA parameters\n",
        "        self.A = nn.Parameter(torch.zeros(r, linear.in_features))\n",
        "        self.B = nn.Parameter(torch.zeros(linear.out_features, r))\n",
        "\n",
        "        # initialize\n",
        "        nn.init.kaiming_uniform_(self.A, a=math.sqrt(5))\n",
        "        nn.init.zeros_(self.B)\n",
        "\n",
        "        # freeze original\n",
        "        for p in self.linear.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, seq_len, in_features)\n",
        "        out = self.linear(x)                       # (batch, seq_len, out_features)\n",
        "        lora_out = (x @ self.A.T) @ self.B.T * self.scaling  # same shape\n",
        "        return out + lora_out\n"
      ],
      "metadata": {
        "id": "dMlEbYT-7Al0"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_configs = [\n",
        "    {\"name\": \"small\",  \"d_model\": 128, \"n_layers\": 2, \"n_heads\": 2, \"d_ff\": 512},\n",
        "    {\"name\": \"medium\", \"d_model\": 256, \"n_layers\": 4, \"n_heads\": 4, \"d_ff\": 1024},\n",
        "    {\"name\": \"large\",  \"d_model\": 384, \"n_layers\": 6, \"n_heads\": 6, \"d_ff\": 1536},\n",
        "]\n"
      ],
      "metadata": {
        "id": "OSPLuDjZ7qmQ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "    \"\"\"Returns total and trainable parameters.\"\"\"\n",
        "    total = sum(p.numel() for p in model.parameters())\n",
        "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    return total, trainable"
      ],
      "metadata": {
        "id": "U6_eYL6K7wA3"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# model = TinyGPT(\n",
        "#     vocab_size=len(tokenizer),\n",
        "#     d_model=256, n_layers=4, n_heads=4,\n",
        "#     d_ff=1024, max_len=block_size,\n",
        "#     dropout=0.1,\n",
        "#     pos_type=\"learned\",  # <-- switch between \"learned\" and \"sin\" (experiment 1)\n",
        "#     lora_r=8        #  <-- set to lora_r = 0 for full fine tuning (experiment 2)\n",
        "# ).to(device)\n",
        "\n",
        "# optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
        "def run_epoch(model, optimizer, loader, train=True):\n",
        "    device = next(model.parameters()).device # Ensures the device is correctly identified\n",
        "    model.train() if train else model.eval()\n",
        "\n",
        "    total_loss, total_tokens = 0, 0\n",
        "\n",
        "    # Use torch.no_grad() for evaluation phases to save memory and computation\n",
        "    context = torch.enable_grad() if train else torch.no_grad()\n",
        "\n",
        "    with context:\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "\n",
        "            # The optimizer.zero_grad() is correctly placed here for per-batch updates\n",
        "            if train:\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass: Assuming model returns (predictions, loss)\n",
        "            _, loss = model(x, y)\n",
        "\n",
        "            if train:\n",
        "                # 1. Backpropagation (Calculates Gradients)\n",
        "                loss.backward()\n",
        "\n",
        "                # 2. Gradient Clipping (Recommended for Transformers)\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "                # 3. Parameter Update\n",
        "                optimizer.step()\n",
        "\n",
        "            # Accumulate loss and token count\n",
        "            total_loss += loss.item() * x.numel()\n",
        "            total_tokens += x.numel()\n",
        "\n",
        "    return total_loss / total_tokens\n",
        "# def run_epoch(model, optimizer, loader, train=True):\n",
        "#     device = next(model.parameters()).device\n",
        "#     model.train() if train else model.eval()\n",
        "\n",
        "#     total_loss, total_tokens = 0, 0\n",
        "#     for x, y in loader:\n",
        "#         x, y = x.to(device), y.to(device)\n",
        "#         if train: optimizer.zero_grad()\n",
        "\n",
        "#         _, loss = model(x, y)\n",
        "\n",
        "#         if train:\n",
        "#             loss.backward()\n",
        "#             torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "#             optimizer.step()\n",
        "\n",
        "#         total_loss += loss.item() * x.numel()\n",
        "#         total_tokens += x.numel()\n",
        "\n",
        "#     return total_loss / total_tokens\n",
        "\n",
        "train_curve = []\n",
        "val_curve = []\n"
      ],
      "metadata": {
        "id": "G4nKPv2jCtpv"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_experiment(config_name, d_model, n_layers, n_heads, d_ff, dropout, pos_type, lora_r, epochs=5):\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    # 1. MODEL INITIALIZATION\n",
        "    model = TinyGPT(\n",
        "        vocab_size=len(tokenizer),\n",
        "        d_model=d_model, n_layers=n_layers, n_heads=n_heads,\n",
        "        d_ff=d_ff, max_len=block_size,\n",
        "        dropout=dropout,\n",
        "        pos_type=pos_type,\n",
        "        lora_r=lora_r\n",
        "    ).to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
        "\n",
        "    # 2. PARAMETER LOGGING (Resource Usage Part 1)\n",
        "    total_params, trainable_params = count_parameters(model)\n",
        "\n",
        "    print(f\"\\n--- Starting Experiment: {config_name} ---\")\n",
        "    print(f\"Trainable Params: {trainable_params:,} ({trainable_params/total_params*100:.2f}%)\")\n",
        "\n",
        "    # 3. TRAINING LOOP and METRIC COLLECTION\n",
        "    train_curve, val_curve, max_mem_GB = [], [], []\n",
        "    total_start_time = time.time()\n",
        "\n",
        "    if device == \"cuda\":\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        t0 = time.time()\n",
        "\n",
        "        # We need a defined run_epoch function (use your existing one)\n",
        "        train_loss = run_epoch(model, optimizer, train_loader, train=True)\n",
        "        val_loss = run_epoch(model, optimizer, val_loader, train=False)\n",
        "\n",
        "        elapsed = time.time() - t0\n",
        "        train_curve.append(train_loss)\n",
        "        val_curve.append(val_loss)\n",
        "\n",
        "        # PEAK GPU MEMORY LOGGING (Resource Usage Part 2)\n",
        "        current_max_memory = 0.0\n",
        "        if device == \"cuda\":\n",
        "            current_max_memory = torch.cuda.max_memory_allocated() / (1024**3)\n",
        "            torch.cuda.reset_peak_memory_stats() # Reset after measurement\n",
        "        max_mem_GB.append(current_max_memory)\n",
        "\n",
        "        print(f\"Epoch {epoch}: val {val_loss:.4f} | ppl {math.exp(val_loss):.1f} | time {elapsed:.1f}s | Mem Peak {current_max_memory:.2f}GB\")\n",
        "\n",
        "    total_time = time.time() - total_start_time\n",
        "    avg_peak_mem = sum(max_mem_GB) / len(max_mem_GB) if max_mem_GB else 0\n",
        "    final_ppl = math.exp(val_loss)\n",
        "\n",
        "    # 4. RETURN ALL RESULTS\n",
        "    results = {\n",
        "        'config_name': config_name,\n",
        "        'd_model': d_model,\n",
        "        'n_layers': n_layers,\n",
        "        'lora_r': lora_r,\n",
        "        'pos_type': pos_type,\n",
        "        'total_params': total_params,\n",
        "        'trainable_params': trainable_params,\n",
        "        'final_val_ppl': final_ppl,\n",
        "        'total_time_s': total_time,\n",
        "        'avg_peak_mem_GB': avg_peak_mem,\n",
        "        'train_curve': train_curve,\n",
        "        'val_curve': val_curve\n",
        "    }\n",
        "    return results"
      ],
      "metadata": {
        "id": "PJsmq400AF5j"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Common Parameters (Ensure these match your actual setup) ---\n",
        "N_HEADS = 4\n",
        "D_FF = 1024\n",
        "DROPOUT = 0.1\n",
        "EPOCHS = 5 # Adjust epochs as needed\n",
        "\n",
        "all_results = []\n",
        "\n",
        "print(\"Starting Comparative Experiments...\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# ==========================================================\n",
        "# --- 1. BASE MODEL (Medium, Full Fine-Tuning, Learned PE) ---\n",
        "# This serves as the reference point (lora_r=0 means Full FT).\n",
        "# ==========================================================\n",
        "base_config = run_experiment(\n",
        "    config_name=\"Base_Medium_FullFT\",\n",
        "    d_model=256, n_layers=4, n_heads=N_HEADS, d_ff=D_FF,\n",
        "    dropout=DROPOUT, pos_type=\"learned\", lora_r=0\n",
        ")\n",
        "all_results.append(base_config)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 30)\n",
        "# ==========================================================\n",
        "# --- 2. EXPERIMENT: MODEL SIZE TRADE-OFF (Small) ---\n",
        "# Compares Base Model vs. a smaller architecture (d=128, L=2).\n",
        "# ==========================================================\n",
        "# NOTE: d_ff and n_heads should be adjusted proportionally for the small model\n",
        "small_config = run_experiment(\n",
        "    config_name=\"Small_FullFT\",\n",
        "    d_model=128, n_layers=2, n_heads=2, d_ff=512,\n",
        "    dropout=DROPOUT, pos_type=\"learned\", lora_r=0\n",
        ")\n",
        "all_results.append(small_config)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 30)\n",
        "# ==========================================================\n",
        "# --- 3. EXPERIMENT: POSITIONAL ENCODING TRADE-OFF (Sinusoidal) ---\n",
        "# Compares Base Model vs. Sinusoidal PE.\n",
        "# ==========================================================\n",
        "sinusoidal_config = run_experiment(\n",
        "    config_name=\"Medium_Sinusoidal\",\n",
        "    d_model=256, n_layers=4, n_heads=N_HEADS, d_ff=D_FF,\n",
        "    dropout=DROPOUT, pos_type=\"sinusoidal\", lora_r=0\n",
        ")\n",
        "all_results.append(sinusoidal_config)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 30)\n",
        "# ==========================================================\n",
        "# --- 4. EXPERIMENT: EFFICIENCY TRADE-OFF (LoRA PEFT) ---\n",
        "# Compares Base Model (Full FT) vs. LoRA PEFT (r=8).\n",
        "# ==========================================================\n",
        "lora_config = run_experiment(\n",
        "    config_name=\"Medium_LoRA_PEFT\",\n",
        "    d_model=256, n_layers=4, n_heads=N_HEADS, d_ff=D_FF,\n",
        "    dropout=DROPOUT, pos_type=\"learned\", lora_r=8\n",
        ")\n",
        "all_results.append(lora_config)\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# --- FINAL STEP: Convert to a DataFrame for Analysis ---\n",
        "# ----------------------------------------------------------\n",
        "import pandas as pd\n",
        "results_df = pd.DataFrame(all_results)\n",
        "\n",
        "print(\"\\n\" * 2 + \"-\" * 40)\n",
        "print(\"--- FINAL SUMMARY TABLE (4 Comparative Experiments) ---\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# This table provides all the data needed for your required discussion\n",
        "print(results_df[[\n",
        "    'config_name',\n",
        "    'trainable_params',\n",
        "    'total_params', # Added total params for clarity on size\n",
        "    'final_val_ppl',\n",
        "    'total_time_s',\n",
        "    'avg_peak_mem_GB'\n",
        "]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWNQb0tOBt29",
        "outputId": "57903aee-59c2-4194-df42-abba0ac931a8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Comparative Experiments...\n",
            "------------------------------\n",
            "\n",
            "--- Starting Experiment: Base_Medium_FullFT ---\n",
            "Trainable Params: 28,923,904 (100.00%)\n",
            "Epoch 0: val 6.5273 | ppl 683.6 | time 90.3s | Mem Peak 7.02GB\n",
            "Epoch 1: val 6.1752 | ppl 480.7 | time 89.6s | Mem Peak 7.02GB\n",
            "Epoch 2: val 5.9618 | ppl 388.3 | time 89.9s | Mem Peak 7.02GB\n",
            "Epoch 3: val 5.8056 | ppl 332.2 | time 90.0s | Mem Peak 7.02GB\n",
            "Epoch 4: val 5.6981 | ppl 298.3 | time 90.0s | Mem Peak 7.02GB\n",
            "\n",
            "==============================\n",
            "\n",
            "--- Starting Experiment: Small_FullFT ---\n",
            "Trainable Params: 13,278,976 (100.00%)\n",
            "Epoch 0: val 6.9248 | ppl 1017.2 | time 50.1s | Mem Peak 6.40GB\n",
            "Epoch 1: val 6.6534 | ppl 775.4 | time 50.5s | Mem Peak 6.40GB\n",
            "Epoch 2: val 6.4526 | ppl 634.4 | time 50.6s | Mem Peak 6.40GB\n",
            "Epoch 3: val 6.3066 | ppl 548.2 | time 50.6s | Mem Peak 6.40GB\n",
            "Epoch 4: val 6.1984 | ppl 492.0 | time 50.7s | Mem Peak 6.40GB\n",
            "\n",
            "==============================\n",
            "\n",
            "--- Starting Experiment: Medium_Sinusoidal ---\n",
            "Trainable Params: 28,891,136 (100.00%)\n",
            "Epoch 0: val 6.3967 | ppl 599.9 | time 89.8s | Mem Peak 7.02GB\n",
            "Epoch 1: val 6.0264 | ppl 414.2 | time 89.9s | Mem Peak 7.02GB\n",
            "Epoch 2: val 5.8226 | ppl 337.8 | time 89.9s | Mem Peak 7.02GB\n",
            "Epoch 3: val 5.6800 | ppl 292.9 | time 90.2s | Mem Peak 7.02GB\n",
            "Epoch 4: val 5.5748 | ppl 263.7 | time 90.3s | Mem Peak 7.02GB\n",
            "\n",
            "==============================\n",
            "\n",
            "--- Starting Experiment: Medium_LoRA_PEFT ---\n",
            "Trainable Params: 27,920,384 (96.37%)\n",
            "Epoch 0: val 6.5941 | ppl 730.7 | time 90.5s | Mem Peak 7.01GB\n",
            "Epoch 1: val 6.2677 | ppl 527.3 | time 90.8s | Mem Peak 7.01GB\n",
            "Epoch 2: val 6.0705 | ppl 432.9 | time 91.0s | Mem Peak 7.01GB\n",
            "Epoch 3: val 5.9358 | ppl 378.4 | time 91.1s | Mem Peak 7.01GB\n",
            "Epoch 4: val 5.8360 | ppl 342.4 | time 91.2s | Mem Peak 7.01GB\n",
            "\n",
            "\n",
            "----------------------------------------\n",
            "--- FINAL SUMMARY TABLE (4 Comparative Experiments) ---\n",
            "----------------------------------------\n",
            "          config_name  trainable_params  total_params  final_val_ppl  \\\n",
            "0  Base_Medium_FullFT          28923904      28923904     298.300136   \n",
            "1        Small_FullFT          13278976      13278976     491.973333   \n",
            "2   Medium_Sinusoidal          28891136      28891136     263.689061   \n",
            "3    Medium_LoRA_PEFT          27920384      28973056     342.390000   \n",
            "\n",
            "   total_time_s  avg_peak_mem_GB  \n",
            "0    449.874876         7.022514  \n",
            "1    252.634678         6.404411  \n",
            "2    450.154654         7.016843  \n",
            "3    454.682171         7.011060  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "# NOTE: This cell assumes TinyGPT class, tokenizer, and train_blocks are already loaded in memory.\n",
        "\n",
        "# --- VITAL FIX: DEFINE DEVICE FIRST! ---\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Plotting on device: {device}\")\n",
        "# ----------------------------------------\n",
        "\n",
        "# --- 1. SET UP THE INTERPRETABLE INPUT ---\n",
        "INTERPRETABLE_SENTENCE = \"Attention is all you need.\"\n",
        "MAX_LEN = 128\n",
        "\n",
        "# Use the GPT-2 tokenizer (tokenizer must be loaded from a previous cell)\n",
        "tokenized_input = tokenizer(\n",
        "    INTERPRETABLE_SENTENCE,\n",
        "    return_tensors=\"pt\",\n",
        "    padding='max_length',\n",
        "    truncation=True,\n",
        "    max_length=MAX_LEN\n",
        ")\n",
        "\n",
        "sample_input_ids = tokenized_input['input_ids'].to(device)\n",
        "tokens = tokenizer.convert_ids_to_tokens(sample_input_ids[0].tolist())\n",
        "\n",
        "\n",
        "# --- 2. MODEL SETUP (Using best performing medium configuration) ---\n",
        "block_size = 128\n",
        "\n",
        "# Load the model structure (TinyGPT class must be loaded from a previous cell)\n",
        "model_to_plot = TinyGPT(\n",
        "    vocab_size=len(tokenizer), d_model=256, n_layers=4, n_heads=4,\n",
        "    d_ff=1024, max_len=block_size, dropout=0.1, pos_type=\"sinusoidal\", lora_r=0\n",
        ").to(device)\n",
        "\n",
        "# NOTE: For a meaningful plot, you must load the weights from your best trained model here.\n",
        "# E.g.: model_to_plot.load_state_dict(torch.load('best_tinygpt_weights.pt'))\n",
        "\n",
        "\n",
        "# --- 3. FORWARD PASS AND EXTRACTION ---\n",
        "TARGET_LAYER = 3  # The 4th layer (index 3) is a high-level layer\n",
        "TARGET_HEAD = 0   # Use head 0 for simplicity\n",
        "\n",
        "model_to_plot.eval()\n",
        "with torch.no_grad():\n",
        "    _ = model_to_plot(sample_input_ids[:, :-1])\n",
        "\n",
        "# Extract the attention matrix\n",
        "# This relies on the extraction logic implemented in your CausalSelfAttention class\n",
        "attention_matrix = model_to_plot.blocks[TARGET_LAYER].attn.attention_weights.numpy()\n",
        "\n",
        "# Determine the actual sequence length (remove padding)\n",
        "seq_len = (sample_input_ids[0] != tokenizer.pad_token_id).sum().item()\n",
        "matrix_size = seq_len - 1\n",
        "\n",
        "# Trim tokens and matrix to the actual sequence length for clean plotting\n",
        "tokens_trimmed = tokens[:matrix_size]\n",
        "attention_matrix = attention_matrix[:matrix_size, :matrix_size]\n",
        "\n",
        "\n",
        "# --- 4. PLOTTING ---\n",
        "plt.figure(figsize=(10, 10))\n",
        "\n",
        "# Create the heatmap\n",
        "im = plt.imshow(attention_matrix, cmap='viridis', origin='upper', aspect='equal')\n",
        "\n",
        "# Set labels using the trimmed, readable tokens\n",
        "plt.xticks(range(matrix_size), tokens_trimmed, rotation=90, fontsize=8)\n",
        "plt.yticks(range(matrix_size), tokens_trimmed, fontsize=8)\n",
        "\n",
        "plt.xlabel('Keys (Attended To)')\n",
        "plt.ylabel('Queries (Attending)')\n",
        "plt.title(f'Self-Attention Heatmap (Layer {TARGET_LAYER}, Head {TARGET_HEAD})')\n",
        "\n",
        "# Add color bar\n",
        "plt.colorbar(im, fraction=0.046, pad=0.04)\n",
        "plt.tight_layout()\n",
        "plt.savefig('interpretable_attention_heatmap.png')\n",
        "print(\"Heatmap generation attempt successful.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 958
        },
        "id": "QRJypAeeX_A-",
        "outputId": "56a1303b-7f94-4f26-ffd3-e03b2843a43c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plotting on device: cuda\n",
            "Heatmap generation attempt successful.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x1000 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oHWZqG79xUhm"
      },
      "outputs": [],
      "source": [
        "# epochs = 5\n",
        "\n",
        "# for epoch in range(epochs):\n",
        "#     t0 = time.time()\n",
        "#     train_loss = run_epoch(train_loader, train=True)\n",
        "#     val_loss = run_epoch(val_loader, train=False)\n",
        "#     elapsed = time.time() - t0\n",
        "\n",
        "#     train_curve.append(train_loss)\n",
        "#     val_curve.append(val_loss)\n",
        "\n",
        "#     print(f\"Epoch {epoch}: train {train_loss:.4f} | val {val_loss:.4f} | ppl {math.exp(val_loss):.1f} | time {elapsed:.1f}s\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n91-QgJPxWt-"
      },
      "outputs": [],
      "source": [
        "# plt.plot(train_curve, label=\"train\")\n",
        "# plt.plot(val_curve, label=\"val\")\n",
        "# plt.xlabel(\"epoch\")\n",
        "# plt.ylabel(\"loss\")\n",
        "# plt.legend()\n",
        "# plt.title(\"TinyGPT Loss Curves\")\n",
        "# # plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}